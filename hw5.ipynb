{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0276f7-9d2f-48ab-8bee-e0f26491d642",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afdaf6b-7995-4a0d-a965-510f0a309b6b",
   "metadata": {},
   "source": [
    "A hypothesis is an idea that can be tested. This differentiates it from other ideas that cannot be empirically examined. The video provides the example of comparing Hillary Clinton and Donald Trump during the 2016 US presidential election. Because neither candidate had yet served as president, there was no data available on their presidential performance, making it impossible to statistically compare them. Conversely, comparing Barack Obama and George W. Bush within a hypothesis testing framework was possible due to the availability of eight years of performance data for both presidents.\r\n",
    "The video uses this example to illustrate a key criterion for a good null hypothesis: it must be testable using available data. A null hypothesis typically represents the \"no effect\" assumption and serves as the starting point for statistical analysis. The goal of hypothesis testing is to gather evidence against the null hypothesis, potentially leading to its rejection and support for the alternative hypothesis, which simply states that the null hypothesis is false.pothesis.\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed832a82-94ed-43a6-8d41-af9b27fd969c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8930667d-b9ac-40c5-90b6-53f13d80f0ec",
   "metadata": {},
   "source": [
    "A p-value is a statistical measure that helps us assess the strength of evidence against a null hypothesis. It quantifies the likelihood of observing data as extreme as the data we actually observed, assuming the null hypothesis is true.\n",
    "\r\n",
    "Think of it like this: the smaller the p-value, the more unlikely it is that we would have observed our data if the null hypothesis were actually true. This suggests that our data might be better explained by an alternative explanation, leading us to potentially reject the null hypothesis\n",
    ".\r\n",
    "For instance, let's say we're testing whether a new drug has an effect on blood pressure. Our null hypothesis would be \"the drug has no effect on blood pressure\". We conduct a study and observe a difference in blood pressure between the group that received the drug and the group that didn't. Now we need to figure out if this observed difference is statistically significant or just due to random chanc\n",
    "e.\r\n",
    "This is where the p-value comes in. It tells us the probability of observing a difference as large as, or even larger than, the one we saw in our study, if the drug truly has no effect (i.e., if the null hypothesis is true). If the p-value is very small, say 0.01, it means there's only a 1% chance of seeing such a large difference in blood pressure if the drug doesn't actually work. This would be considered strong evidence against the null hypothesis, suggesting the drug might indeed have an effe\n",
    "ct.\r\n",
    "On the other hand, if the p-value is relatively large, say 0.25, it means there's a 25% chance of observing our data even if the null hypothesis is true. This would be considered weak evidence against the null hypothesis, and we might not have enough confidence to reject\n",
    " it.\r\n",
    "The sources emphasize that a good understanding of p-values is crucial for interpreting statistical results and making informed decisions based on data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ae8da-1db4-43b9-8f39-3e5aa585d08b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d07ea4-4e1c-4310-abd8-4426bdfb1fdd",
   "metadata": {},
   "source": [
    "The key factor that differentiates testable ideas from untestable ones in statistics is the availability of data that can be analyzed to potentially contradict the idea.  If you can't collect and examine data to see if an idea holds up, then you can't statistically test it.\r\n",
    "\r\n",
    "The sources provide examples to illustrate this:\r\n",
    "\r\n",
    "●We can test a drug's effectiveness because we can collect data on patients' health before and after they take the drug.\r\n",
    "\r\n",
    "●We can test a coin's fairness by flipping it repeatedly and analyzing the heads-tails distribution.\r\n",
    "\r\n",
    "●We can test whether people favour tilting their heads to one side when kissing by observing and recording the behaviour of kissing couples.\r\n",
    "\r\n",
    "However, we couldn't statistically assess a hypothesis about Hillary Clinton and Donald Trump's relative performance as U.S. presidents during the 2016 election.  This is because neither had held the office, meaning there was no data to analyze.\r\n",
    "\r\n",
    "A good null hypothesis generally possesses these key characteristics:\r\n",
    "\r\n",
    "●Testability: There should be a way to gather and analyze data to determine if it's likely false.\r\n",
    "\r\n",
    "●\"No effect\" Assumption: It often represents the assumption of no effect or no difference, serving as a baseline for comparison with the data.\r\n",
    "\r\n",
    "In the \"Vaccine Data Analysis Assignment,\" an appropriate null hypothesis could be: \"the vaccine has no effect (on average) on patient health\".\r\n",
    "\r\n",
    "In essence, the null hypothesis is a statement we assume to be true until we find convincing evidence to the contrary.  Conversely, the alternative hypothesis simply asserts that the null hypothesis is false.  If the data strongly suggest the null hypothesis is unlikely, we might reject it in favour of the alternative hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb033b-6f0b-482d-9273-5b8fb00991b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a78c6-25c0-48c5-830b-b5eb73e4e996",
   "metadata": {},
   "source": [
    "\"It is important to note that outcomes of tests refer to the population parameter, rather than the sample statistic! As such, the result that we get is for the population,\" emphasizes that hypothesis tests aim to draw conclusions about the entire population, not just the specific sample we've collected data from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b564a-2431-4449-b137-741e8b435e4c",
   "metadata": {},
   "source": [
    "\"Outcomes of tests refer to the population parameter...\" This means that when we conduct a hypothesis test, our conclusions are aimed at the population parameter, which is the true value of the characteristic we're studying in the entire population (like $\\mu$, the population mean). \n",
    "\n",
    "\"...rather than the sample statistic!\" Our analysis relies on a sample statistic, which is calculated from the sample data (like $\\bar{x}$, the sample mean). However, we're not solely interested in the specific value of the sample statistic itself. We use it as a tool to make inferences about the larger population.\n",
    "\n",
    "\"As such, the result that we get is for the population.\" This highlights that the ultimate goal of a hypothesis test is to determine whether there is enough evidence from the sample to draw conclusions about the population parameter. If we reject the null hypothesis, we're suggesting that the hypothesized value  ($\\mu_0$) is likely not the true population mean ($\\mu$).  Our result provides insight into the characteristics of the entire population, not just the limited sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e499a5b6-4c42-47f3-b576-63da7b254eed",
   "metadata": {},
   "source": [
    "**Population:**  Imagine a large group of individuals we're interested in studying. For example, if we're examining the effectiveness of a new drug, the population might be all individuals who could potentially benefit from the drug.\n",
    "\n",
    "**Sample:** It's usually impossible to collect data from every single person in the entire population. So, we select a smaller group, called a sample, to represent the population.  Our goal is for the sample to be as representative of the population as possible.\n",
    "\n",
    "**$x_i$ (Individual data points):** This represents the observed value for a specific individual in our sample. For example, if we're measuring blood pressure,  $x_1$ might be the blood pressure of the first person in our sample, $x_2$ the blood pressure of the second person, and so on.\n",
    "\n",
    "**$\\bar{x}$ (Sample mean):** This is the average of all the individual data points ($x_i$ values) in our sample.  It provides a summary of the typical value observed in our sample.\n",
    "\n",
    "**$\\mu$ (Population mean):** This is the average value for the entire population. It's the true value we're trying to learn about through our study.\n",
    "\n",
    "**$\\mu_0$ (Hypothesized population mean):** This is the specific value we're testing in our hypothesis test. The null hypothesis ($H_0$) often states that the population mean ($\\mu$) is equal to this hypothesized value ($\\mu_0$).  For instance, in a drug trial, the null hypothesis might be that the drug has no effect, meaning $\\mu_0$ would represent the average outcome without the drug."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a1ebc-a88c-4783-995a-83c4ae8e62ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8f3f1-0c05-460b-9ece-adfecaecde36",
   "metadata": {},
   "source": [
    "The second \"Pre-lecture\" video's phrase, \"imagine a world where the null hypothesis is true\" when calculating a p-value, refers to the process of simulating data under the assumption that the null hypothesis is correct. This is necessary because the p-value represents the probability of observing a test statistic as extreme as the one obtained from the sample data, if the null hypothesis were actually true.\n",
    "\n",
    "In essence, \"imagining a world where the null hypothesis is true\" involves creating a simulated dataset that reflects the characteristics we would expect if the null hypothesis were correct. This allows us to compare the observed data to this simulated distribution and assess the likelihood of observing such data if the null hypothesis were indeed true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f97e72-28ed-4a5a-a877-2478e07d4637",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3870af3-cbb8-4ac6-9fd3-ba9ca6221dff",
   "metadata": {},
   "source": [
    "A smaller p-value makes the null hypothesis seem less believable because it indicates that the actual data is unlikely to occur if the null hypothesis were correct, making the idea that the null is true seem far-fetched.\n",
    "\n",
    "Null Hypothesis: This is the default assumption we make, which often states that there is \"no effect\" or \"no difference\".\n",
    "\n",
    "P-value: This is the probability of observing data as extreme as what we actually got, assuming the null hypothesis is true. A high p-value means that the observed data could easily have occurred under the null hypothesis, so there's little reason to doubt it.\n",
    "\n",
    "Smaller P-value: When the p-value is small, it means that the observed results are very unlikely to occur if the null hypothesis were true. Essentially, the smaller the p-value, the more surprising or unusual the results are under the assumption that the null hypothesis is correct. This undermines confidence in the null hypothesis.\n",
    "\n",
    "Ridiculousness of the Null: If the results we see are highly improbable under the null hypothesis (because of the low p-value), continuing to believe in the null hypothesis starts to feel unreasonable. The data is telling us that what we observed is not what we’d expect if the null hypothesis were true, so rejecting the null hypothesis becomes more justified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e16a2c9-0277-4b90-9068-66874b4de790",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44337ccc-3cf1-4a86-acdf-256c93ab18f2",
   "metadata": {},
   "source": [
    "1. Understanding the Problem Context:\n",
    "\n",
    "    * **Population:** All kissing couples.\n",
    "    * **Sample:** The 124 couples observed by Güntürkün (2003).\n",
    "    * **$x_i$:**  Each individual observation would be whether a couple tilted their heads to the right (1) or not (0).\n",
    "    * **$\\bar{x}$:** The observed sample proportion, which is 64.5% (80 out of 124 couples tilting their heads to the right).\n",
    "    * **$\\mu$:** The true population proportion of couples who tilt their heads to the right when kissing.\n",
    "    * **$\\mu_0$:** The hypothesized population proportion under the null hypothesis, which is 50% (representing no preference for left or right).\n",
    "\n",
    "2. **Simulating the Null Hypothesis:**\n",
    "\n",
    "   * **\"Imagine a world where the null hypothesis is true\":** This means we assume that there's no real preference for head tilting while kissing. It's like flipping a fair coin - there's a 50/50 chance of getting heads (tilting right) or tails (tilting left).\n",
    "\n",
    "3. **Calculating the p-value:**\n",
    "\n",
    "    * We need to determine how likely it is to observe 80 or more couples tilting their heads to the right out of 124, *if* there's no actual preference (as the null hypothesis suggests). \n",
    "    * We can do this by simulating many coin flips (representing couples) and counting how often we get 80 or more \"heads\" (right tilts) in 124 flips. The proportion of simulations with 80 or more \"heads\" will represent our p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e59146f-cb62-4099-8284-8674fae41cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated p-value: 0.0008\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_simulations = 10000  # Number of coin-flip simulations\n",
    "n_couples = 124        # Sample size \n",
    "right_tilts = 80       # Observed number of right tilts\n",
    "\n",
    "simulated_right_tilts = np.random.binomial(n_couples, 0.5, size=n_simulations)\n",
    "p_value = np.mean(simulated_right_tilts >= right_tilts)\n",
    "\n",
    "print(\"Simulated p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b585c72-9718-4a4d-b788-ef2d39f4a6c5",
   "metadata": {},
   "source": [
    "4. **Interpreting the p-value:**\n",
    "\n",
    "    *  Let's say the simulated p-value is 0.002. This means that in our 10,000 simulations of 124 coin flips, only 0.2% of the simulations resulted in 80 or more heads.  \n",
    "\n",
    "    *  According to the table provided:\n",
    "\n",
    "       | p-value        | Evidence                                                |\n",
    "       | ------        | ------                                                |\n",
    "       |  *p* > 0.1   | No evidence against the null hypothesis              |\n",
    "       | 0.1 ≥  *p* > 0.05 | Weak evidence against the null hypothesis             |\n",
    "       | 0.05 ≥  *p* > 0.01 | Moderate evidence against the null hypothesis        |\n",
    "       | 0.01 ≥  *p* > 0.001 | Strong evidence against the null hypothesis          |\n",
    "       | 0.001 ≥ *p*  | Very strong evidence against the null hypothesis       |\n",
    "\n",
    "    *  A p-value of 0.002 indicates **very strong evidence against the null hypothesis**.  It suggests that it's highly unlikely to observe such a strong preference for right-tilting (64.5%) if there were no real head-tilting tendency in the population.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Based on our simulated p-value, we have very strong evidence to reject the null hypothesis that there is no left or right head-tilt tendency when kissing. The data suggests that there is indeed a preference for tilting the head to the right while kissing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e77b4-b633-4716-a35d-36fc6be8f33c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa99f8-600e-47f6-8cad-d176a944a652",
   "metadata": {},
   "source": [
    "**No, a smaller p-value cannot definitively prove that the null hypothesis is false.** Similarly, it's **impossible to definitively prove Fido's guilt or innocence using a p-value alone.**  \n",
    "\n",
    "Here's why:\n",
    "\n",
    "* **P-values Represent Probabilities, Not Certainties:** A p-value indicates the probability of observing the obtained data (or more extreme data) *if the null hypothesis were true*. It does not directly measure the probability of the null hypothesis itself being true or false.\n",
    "\n",
    "* **Hypothesis Testing is About Evidence, Not Proof:** Statistical hypothesis testing, which uses p-values, aims to provide evidence *against* the null hypothesis. We can either reject the null hypothesis (if the evidence is strong enough) or fail to reject it (if the evidence is weak).  We never \"prove\" anything definitively in hypothesis testing.\n",
    "\n",
    "* **The Example of Fido:** A low p-value in the Fido scenario (e.g., finding a large piece of pie on his face) would provide strong evidence that he ate the pie, but it wouldn't be absolute proof. There's always a chance, however small, that the pie landed on his face in some other highly improbable way. Conversely, a high p-value (e.g., finding a generic breadcrumb) wouldn't definitively prove his innocence. It just means that the evidence is weak and doesn't strongly contradict the assumption of innocence.\n",
    "\n",
    "**There's no specific p-value threshold that guarantees absolute proof.** Even a p-value of 0.0001 (very strong evidence against the null hypothesis) still leaves a 0.01% chance of observing the data if the null hypothesis were true.\n",
    "\n",
    "**The Importance of Context:** The interpretation of a p-value should always consider the context of the problem. Factors such as sample size, study design, and potential alternative explanations for the observed data should be taken into account when drawing conclusions. \n",
    "\n",
    "**In summary, p-values provide a measure of the strength of evidence against a null hypothesis, but they do not offer absolute proof of guilt or innocence or any other definitive conclusion. ** They are a valuable tool in the scientific process but should be interpreted with caution and in conjunction with other relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00bb7d8-377d-4562-b3e9-40f5d4be3121",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfcf14a-c6ee-4b81-9e24-7e130f2bd10d",
   "metadata": {},
   "source": [
    "## One-Tailed Hypothesis Testing for the Vaccine Data\n",
    "\n",
    "Let's work through adjusting the code from \"Demo II of the Week 5 TUT\" to perform a one-sided hypothesis test, and then discuss the changes and implications.\n",
    "\n",
    "### Code Adjustment\n",
    "\n",
    "The original code in \"Demo II of the Week 5 TUT\" calculates a **two-sided p-value**, which means it considers both tails of the sampling distribution under the null hypothesis. Here's how to adjust it for a **one-sided test**:\n",
    "\n",
    "**Original Code (Two-Sided):**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# ... (rest of the code)\n",
    "\n",
    "simulated_stats = np.random.binomial(n_patients, 0.5, size=n_simulations)\n",
    "p_value = np.mean(np.abs(simulated_stats - 0.5) >= np.abs(obs_stat - 0.5))\n",
    "\n",
    "print(\"Simulated p-value (two-sided):\", p_value)\n",
    "```\n",
    "\n",
    "**Adjusted Code (One-Sided, assuming we're interested in p > 0.5):**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# ... (rest of the code)\n",
    "\n",
    "simulated_stats = np.random.binomial(n_patients, 0.5, size=n_simulations)\n",
    "p_value = np.mean(simulated_stats >= obs_stat)  # Change here\n",
    "\n",
    "print(\"Simulated p-value (one-sided):\", p_value)\n",
    "```\n",
    "\n",
    "**Explanation of the Change:**\n",
    "\n",
    "* **Two-Sided:** The original code used `np.abs()` to take the absolute value of the difference between the simulated statistics and the null hypothesis proportion (0.5), as well as the absolute difference between the observed statistic and 0.5. This accounts for both directions of deviation from the null hypothesis.\n",
    "\n",
    "* **One-Sided:**  The adjusted code removes the `np.abs()` function and directly compares the simulated statistics (`simulated_stats`) to the observed statistic (`obs_stat`). We are assuming that we are interested in testing whether the vaccine *improves* health (i.e., `p > 0.5`). So, we only care about the right tail of the sampling distribution (where simulated proportions are greater than or equal to the observed proportion).\n",
    "\n",
    "### Changes in Interpretation\n",
    "\n",
    "The key change in interpretation is the **directionality of the hypothesis test:**\n",
    "\n",
    "* **Two-Sided:** The two-sided test addresses the general question of whether there's *any difference* in patient health due to the vaccine, regardless of whether the difference is positive or negative.\n",
    "\n",
    "* **One-Sided:** The one-sided test specifically addresses whether the vaccine *improves* patient health. It only considers evidence in the direction of improvement (i.e., proportions greater than 0.5).\n",
    "\n",
    "### Expected Change in P-value\n",
    "\n",
    "**Yes, we should generally expect the p-value to be smaller in the one-tailed test compared to the two-tailed test.**  This is because:\n",
    "\n",
    "* The one-tailed test only considers evidence in one direction (the right tail in our example), whereas the two-tailed test splits the evidence across both tails. \n",
    "\n",
    "*  For the same observed statistic, the probability of observing a value as extreme or more extreme in a single tail will be smaller than the probability of observing a value as extreme or more extreme in either of the two tails.\n",
    "\n",
    "### Example\n",
    "\n",
    "Imagine the observed proportion of patients with improved health after the vaccine is 0.6.\n",
    "\n",
    "* **Two-Sided:** The p-value would account for both simulations where the proportion is greater than or equal to 0.6 *and* simulations where it's less than or equal to 0.4. \n",
    "\n",
    "* **One-Sided:** The p-value would only consider simulations where the proportion is greater than or equal to 0.6. \n",
    "\n",
    "This concentration of probability in a single tail generally leads to a smaller p-value in the one-tailed test.\n",
    "\n",
    "### Important Note: Choosing One-Tailed vs. Two-Tailed Tests\n",
    "\n",
    "The choice between a one-tailed and two-tailed test should be made **before** collecting the data, based on the specific research question. \n",
    "\n",
    "* If you're only interested in detecting a difference in one direction, a one-tailed test is appropriate.\n",
    "* If you're interested in detecting any difference, regardless of direction, a two-tailed test is appropriate.\n",
    "\n",
    "It's important to be transparent about your choice of test, as it can influence the interpretation of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb81fc8-d6c1-4cf0-bbf0-73a0060103aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97e623-10b7-4f67-aa56-14008c3a96c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Problem Introduction\n",
    "\n",
    "The experiment we're analyzing is inspired by the famous tea-tasting experiment conducted by Ronald Fisher in the 1920s with his colleague Dr. Muriel Bristol. Fisher proposed this experiment to test Dr. Bristol's claim that she could taste the difference between tea poured first and milk poured first. Fisher's method became one of the foundations of modern hypothesis testing.\n",
    "\n",
    "In this case, we are conducting a similar experiment with a random sample of 80 STA130 students. Each student tastes one cup of tea and indicates whether they think the tea or the milk was poured first. Out of these 80 students, 49 correctly identified which was poured first. We aim to determine whether this result could have occurred by random guessing or if there is statistical evidence that the students can identify the pouring order better than chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d55e58-c041-4932-8b03-5d53ba226cfa",
   "metadata": {},
   "source": [
    "### Hypotheses\n",
    "\n",
    "Null Hypothesis (H₀): The students are guessing randomly, and there is no ability to distinguish between tea-first and milk-first. Mathematically, this means that the probability of a correct guess is 50% (i.e., 𝑝=0.5).\n",
    "\n",
    "Informal interpretation: The students cannot really tell whether the milk or tea was poured first, and they are just guessing, so half of them would be expected to guess correctly by chance.\n",
    "\n",
    "Alternative Hypothesis (H₁): The students are not guessing randomly, and they can identify the pouring order better than chance (i.e., 𝑝>0.5).\n",
    "\n",
    "Informal interpretation: More than half of the students can correctly identify whether the tea or milk was poured first, suggesting they have some ability beyond just guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3078ad19-06b6-4970-9014-71ccba8b71fc",
   "metadata": {},
   "source": [
    "## Quantitative Analysis\n",
    "\n",
    "To test this hypothesis, we will use a hypothesis testing approach for proportions. We’ll estimate the probability of obtaining 49 or more correct answers under the assumption that the students are guessing randomly (i.e., the null hypothesis).\n",
    "\n",
    "The methodology can be broken down as follows:\n",
    "\n",
    "1. Sample Proportion (Observed Test Statistic): The observed sample proportion of correct guesses is:\n",
    "\n",
    "𝑝^ = 49 / 80 = 0.6125\n",
    "\n",
    "2. Population Proportion under H₀: Under the null hypothesis, the population proportion of correct guesses is \n",
    "𝑝0 = 0.5\n",
    "\n",
    "3. Test Statistic: We can calculate a z-score to determine how far the observed sample proportion is from the expected population proportion under the null hypothesis. The z-score is given by:\n",
    "\n",
    "𝑧 = (𝑝^ − 𝑝0) / sqr(𝑝0(1 − 𝑝0) / 𝑛)\n",
    "\n",
    "where:\n",
    "\n",
    "*𝑝^ is the sample proportion,\n",
    "\n",
    "*𝑝0 is the null hypothesis population proportion,\n",
    "\n",
    "*𝑛 is the sample size.\n",
    "\n",
    "4. p-value: The p-value represents the probability of obtaining a result at least as extreme as the observed one under the null hypothesis. If this p-value is small (commonly less than 0.05), we reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843beada-a791-4d6b-8d2e-a25d7ee3cf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0124611797498115, 0.022085672454221217)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Given data\n",
    "n = 80  # sample size\n",
    "successes = 49  # number of correct guesses\n",
    "p_hat = successes / n  # sample proportion\n",
    "p_0 = 0.5  # null hypothesis population proportion\n",
    "\n",
    "# Standard error calculation\n",
    "std_error = np.sqrt((p_0 * (1 - p_0)) / n)\n",
    "\n",
    "# Z-score calculation\n",
    "z_score = (p_hat - p_0) / std_error\n",
    "\n",
    "# p-value calculation for a one-tailed test (we're only interested in p > 0.5)\n",
    "p_value = 1 - stats.norm.cdf(z_score)\n",
    "\n",
    "# Output results\n",
    "z_score, p_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bc1956-7f40-4de8-b9af-06500e6413cd",
   "metadata": {},
   "source": [
    "## Findings and Discussion\n",
    "\n",
    "Z-score: The z-score tells us how many standard deviations away from the expected null hypothesis value our sample proportion is.\n",
    "\n",
    "P-value: The p-value helps us assess the likelihood that our observed result could have occurred by random chance.\n",
    "\n",
    "If the p-value is smaller than the common significance threshold (e.g., 0.05), we reject the null hypothesis and conclude that the students likely have some ability to distinguish the pouring order.\n",
    "\n",
    "If the p-value is large, we fail to reject the null hypothesis and conclude that the students’ performance is consistent with random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f4365a-e54a-45c4-9f23-84192a585a16",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Based on the calculated p-value, we can determine whether the STA130 students in this experiment are able to identify the pouring order of tea and milk better than chance or if their performance can be attributed to random guessing. This analysis can be extended with bootstrapping or confidence intervals for further validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d19849-4a8e-4fc2-a58d-ff6f435dd260",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa26e73-709b-46cd-9455-2ee6ce9ff9ab",
   "metadata": {},
   "source": [
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c21fa0-7c8c-41f6-9620-6f8b40e1e2d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# STA130 Custom NBLM Chatbot conversation summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db51b1-24df-405d-9a9c-0fd134c8bf12",
   "metadata": {},
   "source": [
    "This conversation began with you providing some Python code that resulted in a `NameError`. You then asked two conceptual questions related to hypothesis testing. \r\n",
    "\r\n",
    "### Troubleshooting the `NameError`\r\n",
    "\r\n",
    "You encountered a `NameError` while trying to run some Python code. The error message you received was \"NameError: name 'python' is not defined.\" This error likely occurred because the first line of the provided code was `1 python`. This line of code doesn't appear to have a purpose and is likely unnecessary. The error was resolved by removing this line of code.\r\n",
    "\r\n",
    "### Can a p-value prove a hypothesis?\r\n",
    "\r\n",
    "You asked whether a smaller p-value can prove a null hypothesis is false and if a p-value can prove someone's guilt or innocence. \r\n",
    "\r\n",
    "A p-value cannot definitively prove or disprove a null hypothesis. It is important to remember that a p-value is the probability of observing the data if the null hypothesis is true. A low p-value indicates the data is unlikely to have occurred by chance alone if the null hypothesis is true, which provides evidence against the null hypothesis. A high p-value indicates the data is likely to have occurred if the null hypothesis is true, which does not provide evidence against the null hypothesis. However, a p-value doesn't offer definitive proof.\r\n",
    "\r\n",
    "### One-tailed vs. two-tailed hypothesis tests\r\n",
    "\r\n",
    "You also asked about one-tailed versus two-tailed hypothesis tests. \r\n",
    "\r\n",
    "A two-tailed hypothesis test is used to determine if there is a difference between two groups, regardless of the direction of the difference. A one-tailed hypothesis test is used to determine if there is a difference between two groups in a specific direction. For example, a one-tailed hypothesis test could be used to determine if a new drug increases the average lifespan of patients.\r\n",
    "\r\n",
    "You asked about the code used to calculate the p-value for the one-tailed and two-tailed tests in Demo II of the Week 5 Tutorial, which revisits the Vaccine Data Analysis Assignment from Week 4 Homework \"Question 8\". The original code calculates a two-sided p-value using the absolute value function, `np.abs()`. This means that differences in both directions are taken into account. To adjust the code to calculate a one-sided p-value, the `np.abs()` function can be removed. For example, if the one-sided test is assessing if the proportion is greater than 0.5, then the code `p_value = np.mean(simulated_stats >= obs_stat)` would be used to calculate the p-value. This ensures that only differences in one direction are considered.\r\n",
    "\r\n",
    "You also asked if the p-value is expected to be smaller in the one-tailed test. Generally, the p-value will be smaller in the one-tailed test because it only considers differences in one direction, while the two-tailed test splits the evidence across both tails of the distribution.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
